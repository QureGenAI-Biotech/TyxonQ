{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dd80d43",
   "metadata": {},
   "source": [
    "# 梯度和变分优化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66167c3f",
   "metadata": {},
   "source": [
    "## 概述\n",
    "\n",
    "TyxonQ 旨在使参数化量子门的优化变得简单、快速和方便。 在本说明中，我们回顾了如何获得电路梯度和运行变分优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97754f01",
   "metadata": {},
   "source": [
    "## 设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af689b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as optimize\n",
    "import torch\n",
    "import tyxonq as tq\n",
    "\n",
    "K = tq.set_backend(\"pytorch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8d0487",
   "metadata": {},
   "source": [
    "## PQC(Parameterized Quantum Circuit)\n",
    "\n",
    "考虑一个作用于 $n$ 个量子比特的变分电路，由 $k$ 层组成，其中每一层包含相邻量子比特之间的参数化 $e^{i\\theta X\\otimes X}$ 门，\n",
    "然后是一系列参数化的单量子比特 $Z$ 和 $X$ 旋转。\n",
    "我们现在展示如何在 TyxonQ 中实现此类电路，以及如何使用机器学习后端之一轻松高效地计算损失函数和梯度。\n",
    "\n",
    "一般$n,k$的电路和参数集可以定义如下：:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cef70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qcircuit(n, k, params):\n",
    "    c = tq.Circuit(n)\n",
    "    for j in range(k):\n",
    "        for i in range(n - 1):\n",
    "            c.exp1(\n",
    "                i, i + 1, theta=params[j * (3 * n - 1) + i], unitary=tq.gates._xx_matrix\n",
    "            )\n",
    "        for i in range(n):\n",
    "            c.rz(i, theta=params[j * (3 * n - 1) + n - 1 + i])\n",
    "            c.rx(i, theta=params[j * (3 * n - 1) + 2 * n - 1 + i])\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae87b1a7",
   "metadata": {},
   "source": [
    "举个例子，我们取 $n=3, k=2$ ，设置 PyTorch 作为我们的后端，定义一个能量损失函数来最小化\n",
    "$$E = \\langle X_0 X_1\\rangle_\\theta + \\langle X_1 X_2\\rangle_\\theta.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c75e7d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "k = 2\n",
    "\n",
    "\n",
    "def energy(params):\n",
    "    c = qcircuit(n, k, params)\n",
    "    e = c.expectation_ps(x=[0, 1]) + c.expectation_ps(x=[1, 2])\n",
    "    return K.real(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08baf09f",
   "metadata": {},
   "source": [
    "## 梯度和即时编译"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2390d0c5",
   "metadata": {},
   "source": [
    "使用 ML(Machine Learning) 后端对自动微分的支持，我们现在可以快速计算能量和能量相对于参数的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5aedf3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_val_grad = K.value_and_grad(energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27da172d",
   "metadata": {},
   "source": [
    "这将创建一个函数，给定一组参数作为输入，返回能量和能量梯度。如果只需要梯度，则可以通过 ``K.grad(energy)`` 计算。\n",
    "虽然我们可以直接在一组参数上运行上述代码，但如果要对能量进行多次评估，则可以通过使用该函数的即时编译版本来节省大量时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bd4f9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_val_grad_jit = K.jit(energy_val_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdff95f",
   "metadata": {},
   "source": [
    "使用 ``K.jit``，能量和梯度的初始评估可能需要更长的时间，但随后的评估将明显快于非 jitted 代码。\n",
    "我们建议始终使用 ``jit``，只要函数是 ``张量输入，张量输出`` 的形式，我们已经努力使电路模拟器的各个方面都与 JIT 兼容。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dbd1a5",
   "metadata": {},
   "source": [
    "## 通过 ML(Machine Learning)  后端进行优化\n",
    "\n",
    "有了可用的能量函数和梯度，参数的优化就很简单了。下面是一个如何通过随机梯度下降来做到这一点的例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1283344e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0, energy=-0.670495331287384\n",
      "i=10, energy=-1.2501306533813477\n",
      "i=20, energy=-1.4198601245880127\n",
      "i=30, energy=-1.5485260486602783\n",
      "i=40, energy=-1.6695808172225952\n",
      "i=50, energy=-1.7761402130126953\n",
      "i=60, energy=-1.8588359355926514\n",
      "i=70, energy=-1.915252923965454\n",
      "i=80, energy=-1.9499529600143433\n",
      "i=90, energy=-1.9699469804763794\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 2e-2\n",
    "# warning pytorch might be unable to do this exactly\n",
    "params = torch.nn.Parameter(torch.randn(k * (3 * n - 1)))\n",
    "opt = torch.optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "\n",
    "def grad_descent(params, i):\n",
    "    val, grad = energy_val_grad_jit(params)\n",
    "    params.grad = grad\n",
    "    opt.step()\n",
    "    if i % 10 == 0:\n",
    "        print(f\"i={i}, energy={val}\")\n",
    "    return params\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    params = grad_descent(params, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97011f8",
   "metadata": {},
   "source": [
    "## 通过 Scipy 界面进行优化\n",
    "\n",
    "使用机器学习后端进行优化的另一种方法是使用 SciPy。\n",
    "这可以通过 ``scipy_interface`` API 调用来完成，并允许使用基于梯度（例如 BFGS）和非基于梯度（例如 COBYLA）的优化器，这在 ML(Machine Learning)  后端是不可用的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cc8845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: -2.0000014305114746\n",
       "        x: [-7.126e-01 -2.315e+00 ...  1.007e+00  5.574e-01]\n",
       "      nit: 16\n",
       "      jac: [ 7.153e-05  1.353e-04 ...  0.000e+00  0.000e+00]\n",
       "     nfev: 50\n",
       "     njev: 50\n",
       " hess_inv: <16x16 LbfgsInvHessProduct with dtype=float64>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_scipy = tq.interfaces.scipy_interface(energy, shape=[k * (3 * n - 1)], jit=True)\n",
    "params = torch.randn(k * (3 * n - 1)).numpy()\n",
    "r = optimize.minimize(f_scipy, params, method=\"L-BFGS-B\", jac=True)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6041d1e7",
   "metadata": {},
   "source": [
    "上面的第一行指定了要提供给要最小化的函数的参数的形状，这里是能量函数。\n",
    "``jit=True`` 参数会自动处理能量函数的即时编译。 通过将 ``gradient=False`` 参数提供给``scipy_interface``，同样可以有效地执行无梯度优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f09d59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " message: Return from COBYLA because the trust region radius reaches its lower bound.\n",
       " success: True\n",
       "  status: 0\n",
       "     fun: -1.9999998807907104\n",
       "       x: [ 7.857e-01  7.854e-01 ...  2.270e-01  6.234e-01]\n",
       "    nfev: 278\n",
       "   maxcv: 0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_scipy = tq.interfaces.scipy_interface(\n",
    "    energy, shape=[k * (3 * n - 1)], jit=True, gradient=False\n",
    ")\n",
    "params = torch.randn(k * (3 * n - 1)).numpy()\n",
    "r = optimize.minimize(f_scipy, params, method=\"COBYLA\")\n",
    "r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
