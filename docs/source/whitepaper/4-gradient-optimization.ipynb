{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dd80d43",
   "metadata": {},
   "source": [
    "# Gradient and Variational Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66167c3f",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "TyxonQ is designed to make optimization of parameterized quantum gates easy, fast, and convenient. In this note, we review how to obtain circuit gradients and run variational optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97754f01",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6af689b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as optimize\n",
    "import tensorflow as tf\n",
    "import tyxonq as tq\n",
    "\n",
    "K = tq.set_backend(\"tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8d0487",
   "metadata": {},
   "source": [
    "## PQC(Parameterized Quantum Circuit)\n",
    "\n",
    " Consider a variational circuit acting on $n$ qubits, and consisting of $k$ layers, where each layer comprises parameterized $e^{i\\theta X\\otimes X}$ gates between neighboring qubits followed by a sequence of single qubit parameterized $Z$ and $X$ rotations. We now show how to implement such circuits in TyxonQ, and how to use one of the machine learning backends to compute cost functions and gradients easily and efficiently.\n",
    "\n",
    "The circuit for general $n,k$ and set of parameters can be defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21cef70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qcircuit(n, k, params):\n",
    "    c = tq.Circuit(n)\n",
    "    for j in range(k):\n",
    "        for i in range(n - 1):\n",
    "            c.exp1(\n",
    "                i, i + 1, theta=params[j * (3 * n - 1) + i], unitary=tq.gates._xx_matrix\n",
    "            )\n",
    "        for i in range(n):\n",
    "            c.rz(i, theta=params[j * (3 * n - 1) + n - 1 + i])\n",
    "            c.rx(i, theta=params[j * (3 * n - 1) + 2 * n - 1 + i])\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae87b1a7",
   "metadata": {},
   "source": [
    "As an example, we take $n=3, k=2$, set TensorFlow as our backend, and define an energy cost function to minimize\n",
    "$$E = \\langle X_0 X_1\\rangle_\\theta + \\langle X_1 X_2\\rangle_\\theta.$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c75e7d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "k = 2\n",
    "\n",
    "\n",
    "def energy(params):\n",
    "    c = qcircuit(n, k, params)\n",
    "    e = c.expectation_ps(x=[0, 1]) + c.expectation_ps(x=[1, 2])\n",
    "    return K.real(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08baf09f",
   "metadata": {},
   "source": [
    "## Grad and JIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2390d0c5",
   "metadata": {},
   "source": [
    "Using the ML(Machine Learning) backend support for automatic differentiation, we can now quickly compute both the energy and the gradient of the energy with respect to the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5aedf3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_val_grad = K.value_and_grad(energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27da172d",
   "metadata": {},
   "source": [
    "This creates a function that given a set of parameters as input, returns both the energy and the gradient of the energy. If only the gradient is desired, then this can be computed by ``K.grad(energy)``. While we could run the above code directly on a set of parameters, if multiple evaluations of the energy will be performed, significant time savings can be had by using a just-in-time compiled version of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bd4f9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_val_grad_jit = K.jit(energy_val_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdff95f",
   "metadata": {},
   "source": [
    "With ``K.jit``, the initial evaluation of the energy and gradient may take longer, but subsequent evaluations will be noticeably faster than non-jitted code. We recommend always using ``jit`` as long as the function is \"tensor-in, tensor-out\", and we have worked hard to make all aspects of the circuit simulator compatible with JIT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dbd1a5",
   "metadata": {},
   "source": [
    "## Optimization via ML(Machine Learning) Backend\n",
    "\n",
    "With the energy function and gradients available, optimization of the parameters is straightforward.  Below is an example of how to do this via stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1283344e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0, energy=0.8478720188140869\n",
      "i=10, energy=0.48806655406951904\n",
      "i=20, energy=-0.21994608640670776\n",
      "i=30, energy=-0.8568923473358154\n",
      "i=40, energy=-1.1428836584091187\n",
      "i=50, energy=-1.289249300956726\n",
      "i=60, energy=-1.3814536333084106\n",
      "i=70, energy=-1.451552391052246\n",
      "i=80, energy=-1.5181822776794434\n",
      "i=90, energy=-1.5902677774429321\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 2e-2\n",
    "opt = K.optimizer(tf.keras.optimizers.SGD(learning_rate))\n",
    "\n",
    "\n",
    "def grad_descent(params, i):\n",
    "    val, grad = energy_val_grad_jit(params)\n",
    "    params = opt.update(grad, params)\n",
    "    if i % 10 == 0:\n",
    "        print(f\"i={i}, energy={val}\")\n",
    "    return params\n",
    "\n",
    "\n",
    "params = K.implicit_randn(k * (3 * n - 1))\n",
    "for i in range(100):\n",
    "    params = grad_descent(params, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97011f8",
   "metadata": {},
   "source": [
    "## Optimization via Scipy Interface\n",
    "\n",
    "An alternative to using the machine learning backends for the optimization is to use SciPy.\n",
    "This can be done via the ``scipy_interface`` API call and allows for gradient-based (e.g. BFGS) and non-gradient-based (e.g. COBYLA) optimizers to be used, which are not available via the ML(Machine Learning) backends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4cc8845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: -2.0000009536743164\n",
       "        x: [ 9.246e-01 -7.569e-01 ... -1.089e+00 -9.391e-01]\n",
       "      nit: 27\n",
       "      jac: [ 1.149e-04 -2.503e-06 ...  5.960e-08 -1.192e-07]\n",
       "     nfev: 61\n",
       "     njev: 61\n",
       " hess_inv: <16x16 LbfgsInvHessProduct with dtype=float64>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_scipy = tq.interfaces.scipy_interface(energy, shape=[k * (3 * n - 1)], jit=True)\n",
    "params = K.implicit_randn(k * (3 * n - 1))\n",
    "r = optimize.minimize(f_scipy, params, method=\"L-BFGS-B\", jac=True)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6041d1e7",
   "metadata": {},
   "source": [
    "The first line above specifies the shape of the parameters to be supplied to the function to be minimized, which here is the energy function.  The ``jit=True`` argument automatically takes care of jitting the energy function.  Gradient-free optimization can similarly be performed efficiently by supplying the ``gradient=False`` argument to  ``scipy_interface``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f09d59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " message: Return from COBYLA because the trust region radius reaches its lower bound.\n",
       " success: True\n",
       "  status: 0\n",
       "     fun: -1.9999802112579346\n",
       "       x: [-7.794e-01  1.571e+00 ...  6.284e-01  1.690e+00]\n",
       "    nfev: 514\n",
       "   maxcv: 0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_scipy = tq.interfaces.scipy_interface(\n",
    "    energy, shape=[k * (3 * n - 1)], jit=True, gradient=False\n",
    ")\n",
    "params = K.implicit_randn(k * (3 * n - 1))\n",
    "r = optimize.minimize(f_scipy, params, method=\"COBYLA\")\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f43a6d4-b491-4091-982e-a3183605456e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
