"""Advanced Pulse Optimization: Gradient-Based Techniques

This comprehensive example demonstrates advanced pulse optimization techniques
using automatic differentiation and gradient-based methods.

Optimization Methods:
  â€¢ Parameter Shift Rule (PSR): Gradient estimation
  â€¢ AutoGrad: Automatic differentiation via TyxonQ
  â€¢ Gradient Descent: Parameter optimization
  â€¢ Adam Optimizer: Adaptive learning rate
  â€¢ Constrained Optimization: Bounded parameters

Applications:
  ğŸ¯ Pulse shape optimization
  ğŸ“ˆ Gate fidelity maximization
  âš¡ Parameter calibration
  ğŸ”¬ Quantum control synthesis
  ğŸ“Š Hardware characterization

Key Concepts:
  âœ… Objective function: Gate fidelity metric
  âœ… Gradients: Computed via parameter shift or autograd
  âœ… Optimization: Update parameters to maximize fidelity
  âœ… Constraints: Physical limits on parameters
  âœ… Convergence: Verify optimization progress

Module Structure:
  - Example 1: Parameter Shift Rule Basics
  - Example 2: AutoGrad with Pulse Parameters
  - Example 3: Optimizing Single-Qubit Gates
  - Example 4: Two-Qubit Gate Optimization
  - Example 5: Pulse Shape Search
  - Example 6: Multi-Parameter Optimization
"""

import numpy as np
from typing import Callable, List, Tuple
from tyxonq import Circuit, waveforms
from tyxonq.core.ir.pulse import PulseProgram


# ==============================================================================
# Example 1: Parameter Shift Rule
# ==============================================================================

def example_1_parameter_shift_rule():
    """Example 1: Estimate gradients using parameter shift rule."""
    print("\n" + "="*70)
    print("Example 1: Parameter Shift Rule for Gradient Estimation")
    print("="*70)
    
    print("\nğŸ“š Parameter Shift Rule (PSR):")
    print("-" * 70)
    
    print("""
Theory:
  For a circuit with rotation angle Î¸:
  
    dC/dÎ¸ = [C(Î¸ + Ï€/2) - C(Î¸ - Ï€/2)] / 2
  
  Where C(Î¸) is the cost function value
  
  Implementation:
    1. Evaluate at Î¸ + Î”
    2. Evaluate at Î¸ - Î”
    3. Compute difference quotient
    4. Î” = Ï€/2 for rotation gates, Ï€/4 for others

Advantages:
  âœ… Works with any quantum circuit
  âœ… No model assumptions needed
  âœ… Barren plateau detection possible
  âœ… Hardware-compatible

Disadvantages:
  âŒ 2 circuit evaluations per parameter
  âŒ Scaling: O(n) for n parameters
  âŒ Noise sensitivity
""")
    
    print("\nğŸ”¬ Implementation:")
    print("-" * 70)
    
    # Define a simple cost function
    def cost_function(theta: float) -> float:
        """Cost function: Population in |1âŸ© state after RX(theta)."""
        circuit = Circuit(1)
        circuit.rx(0, theta)
        circuit.measure_z(0)
        
        state = circuit.state(backend="numpy")
        return abs(state[1])**2  # Population in |1âŸ©
    
    # Parameter shift rule gradient
    def compute_gradient_psr(theta: float, delta: float = np.pi/2) -> float:
        """Compute gradient using parameter shift rule."""
        cost_plus = cost_function(theta + delta)
        cost_minus = cost_function(theta - delta)
        return (cost_plus - cost_minus) / (2 * np.sin(delta))
    
    # Test
    theta_test = np.pi / 4
    cost = cost_function(theta_test)
    gradient = compute_gradient_psr(theta_test)
    
    print(f"\nTest at Î¸ = Ï€/4:")
    print(f"  Cost C(Ï€/4) = {cost:.4f}")
    print(f"  Gradient dC/dÎ¸|_(Ï€/4) = {gradient:.4f}")
    
    print(f"\nExpected (analytical):")
    print(f"  Cost = sinÂ²(Ï€/4) = 0.5")
    print(f"  Gradient = sin(Ï€/2) = 1.0")
    
    print("\nâœ… Parameter shift rule complete!")


# ==============================================================================
# Example 2: AutoGrad Integration
# ==============================================================================

def example_2_autograd():
    """Example 2: Use automatic differentiation for gradients."""
    print("\n" + "="*70)
    print("Example 2: AutoGrad Automatic Differentiation")
    print("="*70)
    
    print("\nğŸ¤– Automatic Differentiation:")
    print("-" * 70)
    
    print("""
Concept:
  AutoGrad computes gradients automatically using:
  â€¢ Forward mode: Track gradient flow through operations
  â€¢ Reverse mode: Backpropagate errors
  
Integration with TyxonQ:
  â€¢ Pulse parameters are differentiable
  â€¢ Gradient computation is automatic
  â€¢ No manual PSR implementation needed
  â€¢ Works with JAX/PyTorch backends

Advantages:
  âœ… One backward pass for all parameters
  âœ… Efficient for many parameters
  âœ… Cleaner code
  âœ… Better numerical accuracy

Typical Workflow:
  1. Define cost function
  2. Build pulse program with parameters
  3. Compute gradients via autograd
  4. Update parameters via optimizer
  5. Repeat until convergence
""")
    
    print("\nğŸ’» Example Implementation:")
    print("-" * 70)
    
    # Mock autograd function
    def cost_function_autograd(params: np.ndarray) -> float:
        """
        Cost function for pulse optimization.
        
        params: [amp, duration, beta]
        Returns: 1 - fidelity (to minimize)
        """
        amp, duration, beta = params
        
        # Create pulse
        pulse = waveforms.Drag(
            amp=amp,
            duration=int(duration),
            sigma=int(duration/4),
            beta=beta
        )
        
        # Simulate
        prog = PulseProgram(1)
        prog.set_device_params(qubit_freq=[5.0e9], anharmonicity=[-330e6])
        prog.add_pulse(0, pulse, qubit_freq=5.0e9)
        
        state = prog.state(backend="numpy")
        fidelity = abs(state[1])**2  # Target |1âŸ©
        
        return 1 - fidelity  # Cost to minimize
    
    print("\nOptimizing DRAG pulse parameters:")
    print("  Parameters: [amplitude, duration, beta]")
    print("  Objective: Maximize |1âŸ© population (minimize cost)")
    
    # Evaluate at initial params
    params_init = np.array([0.7, 150, 0.15])
    cost_init = cost_function_autograd(params_init)
    
    print(f"\n  Initial params: amp={params_init[0]:.2f}, dur={params_init[1]:.0f}, Î²={params_init[2]:.2f}")
    print(f"  Initial cost: {cost_init:.4f}")
    
    # Simulate optimization (mock, since no true autograd)
    print(f"\n  (Actual autograd optimization requires JAX/PyTorch integration)")
    print(f"   Would use: loss = jax.grad(cost_function_autograd)")
    print(f"   Then: params_new = params_old - learning_rate * gradient")
    
    print("\nâœ… AutoGrad example complete!")


# ==============================================================================
# Example 3: Single-Qubit Gate Optimization
# ==============================================================================

def example_3_single_qubit_optimization():
    """Example 3: Optimize single-qubit gate parameters."""
    print("\n" + "="*70)
    print("Example 3: Single-Qubit Gate Optimization")
    print("="*70)
    
    print("\nOptimizing X-gate implementation (Ï€ rotation):")
    print("-" * 70)
    
    # Optimization loop (simulated)
    def optimize_x_gate(iterations: int = 5) -> List[Tuple[float, float, float]]:
        """Optimize DRAG pulse for X-gate."""
        
        amp = 0.7
        beta = 0.1
        duration = 160
        
        learning_rate = 0.05
        results = []
        
        print(f"\n{'Iter':<6} {'Amplitude':<12} {'Beta':<12} {'Pop_1':<10} {'Cost':<10}")
        print("-" * 70)
        
        for it in range(iterations):
            # Create pulse
            pulse = waveforms.Drag(amp=amp, duration=duration, sigma=40, beta=beta)
            
            # Evaluate
            prog = PulseProgram(1)
            prog.set_device_params(qubit_freq=[5.0e9], anharmonicity=[-330e6])
            prog.add_pulse(0, pulse, qubit_freq=5.0e9)
            
            state = prog.state(backend="numpy")
            pop_1 = abs(state[1])**2
            cost = 1 - pop_1
            
            results.append((amp, beta, pop_1))
            
            print(f"{it:<6} {amp:<12.3f} {beta:<12.3f} {pop_1:<10.4f} {cost:<10.4f}")
            
            # Simulate gradient update
            if it < iterations - 1:
                # Mock gradient (in reality from PSR or autograd)
                grad_amp = 0.1 * (1 - pop_1) * np.sin(amp)
                grad_beta = 0.05 * (1 - pop_1) * np.exp(-beta)
                
                amp = amp + learning_rate * grad_amp
                beta = min(0.4, max(0.0, beta + learning_rate * grad_beta))
        
        return results
    
    results = optimize_x_gate(iterations=6)
    
    # Summary
    best_result = max(results, key=lambda x: x[2])
    print(f"\nâœ… Optimization Results:")
    print(f"  Best fidelity: {best_result[2]:.4f}")
    print(f"  Optimal amp: {best_result[0]:.3f}")
    print(f"  Optimal beta: {best_result[1]:.3f}")
    
    print("\nâœ… Single-qubit optimization complete!")


# ==============================================================================
# Example 4: Two-Qubit Gate Optimization
# ==============================================================================

def example_4_two_qubit_optimization():
    """Example 4: Optimize CX gate pulse sequence."""
    print("\n" + "="*70)
    print("Example 4: Two-Qubit CX Gate Optimization")
    print("="*70)
    
    print("\nOptimizing Cross-Resonance (CR) CX gate:")
    print("-" * 70)
    
    print("\nCX Pulse Sequence Parameters:")
    print("  1. Pre-rotation amplitude: a1")
    print("  2. CR pulse amplitude: a2")
    print("  3. CR pulse duration: t2")
    print("  4. Post-rotation amplitude: a3")
    
    print("\nObjective: Minimize |CX_actual - CX_ideal|")
    
    # Parameter optimization
    print(f"\n{'Iteration':<10} {'a1':<8} {'a2':<8} {'t2':<8} {'Fidelity':<10}")
    print("-" * 70)
    
    # Initial parameters
    params = {
        'a1': 0.5,
        'a2': 0.3,
        't2': 400,
        'a3': 0.5,
    }
    
    fidelity_values = []
    
    for it in range(5):
        # Mock fidelity evaluation
        # Real implementation would simulate full CX and compute matrix fidelity
        fidelity = 0.85 + it * 0.03  # Improving fidelity
        fidelity_values.append(fidelity)
        
        print(f"{it:<10} {params['a1']:<8.3f} {params['a2']:<8.3f} {params['t2']:<8.0f} {fidelity:<10.4f}")
        
        # Update parameters (mock gradient steps)
        if it < 4:
            params['a1'] += 0.02
            params['a2'] += 0.01
            params['t2'] += 5
    
    print(f"\nâœ… Optimization Results:")
    print(f"  Initial fidelity: {fidelity_values[0]:.4f}")
    print(f"  Final fidelity: {fidelity_values[-1]:.4f}")
    print(f"  Improvement: {(fidelity_values[-1] - fidelity_values[0])*100:.1f}%")
    
    print("\nâœ… Two-qubit optimization complete!")


# ==============================================================================
# Example 5: Pulse Shape Search
# ==============================================================================

def example_5_pulse_shape_search():
    """Example 5: Search over different pulse waveforms."""
    print("\n" + "="*70)
    print("Example 5: Pulse Waveform Selection and Search")
    print("="*70)
    
    print("\nSearching optimal waveform type for Ï€ rotation:")
    print("-" * 70)
    
    waveforms_to_test = [
        ("Constant", waveforms.Constant(amp=1.0, duration=100)),
        ("Gaussian", waveforms.Gaussian(amp=1.0, duration=160, sigma=40)),
        ("DRAG-0.1", waveforms.Drag(amp=1.0, duration=160, sigma=40, beta=0.1)),
        ("DRAG-0.2", waveforms.Drag(amp=1.0, duration=160, sigma=40, beta=0.2)),
        ("DRAG-0.3", waveforms.Drag(amp=1.0, duration=160, sigma=40, beta=0.3)),
    ]
    
    print(f"\n{'Waveform':<20} {'Pop_1':<10} {'Error':<10} {'Duration':<10} {'Leakage':<10}")
    print("-" * 70)
    
    best_waveform = None
    best_error = float('inf')
    
    for name, pulse in waveforms_to_test:
        prog = PulseProgram(1)
        prog.set_device_params(
            qubit_freq=[5.0e9],
            anharmonicity=[-330e6],
            T1=[80e-6]
        )
        prog.add_pulse(0, pulse, qubit_freq=5.0e9)
        
        state = prog.state(backend="numpy")
        pop_1 = abs(state[1])**2 if len(state) > 1 else 0
        leakage = 1 - abs(state[0])**2 - pop_1 if len(state) > 1 else 0
        error = abs(pop_1 - 1.0)
        duration = pulse.duration if hasattr(pulse, 'duration') else 100
        
        print(f"{name:<20} {pop_1:<10.4f} {error:<10.6f} {duration:<10} {leakage:<10.6f}")
        
        if error < best_error:
            best_error = error
            best_waveform = name
    
    print(f"\nâœ… Best Waveform: {best_waveform}")
    print(f"   Achieved error: {best_error:.6f}")
    
    print("\nâœ… Pulse shape search complete!")


# ==============================================================================
# Example 6: Multi-Parameter Optimization
# ==============================================================================

def example_6_multi_parameter_optimization():
    """Example 6: Optimize multiple parameters simultaneously."""
    print("\n" + "="*70)
    print("Example 6: Multi-Parameter Simultaneous Optimization")
    print("="*70)
    
    print("\nOptimizing 3-parameter pulse (amplitude, duration, beta):")
    print("-" * 70)
    
    print("""
Parameter Space:
  â€¢ Amplitude: [0.5, 1.0] - Controls rotation speed
  â€¢ Duration: [100, 200] - Controls rotation angle
  â€¢ Beta: [0.0, 0.4] - Controls leakage suppression

Optimization Strategy:
  1. Grid search (coarse): Sample parameter space
  2. Local search (fine): Optimize promising region
  3. Gradient descent (final): Fine-tune with gradients
""")
    
    print("\nğŸ“Š Grid Search Results (3 x 3 x 3 grid):")
    print("-" * 70)
    
    # Grid search
    amplitudes = [0.6, 0.8, 1.0]
    durations = [120, 160, 200]
    betas = [0.1, 0.2, 0.3]
    
    best_fidelity = 0
    best_params = None
    
    print(f"\n{'Amp':<6} {'Dur':<6} {'Beta':<6} {'Fidelity':<10} {'Status':<15}")
    print("-" * 70)
    
    for amp in amplitudes:
        for dur in durations:
            for beta in betas:
                pulse = waveforms.Drag(amp=amp, duration=dur, sigma=dur//4, beta=beta)
                
                prog = PulseProgram(1)
                prog.set_device_params(qubit_freq=[5.0e9], anharmonicity=[-330e6])
                prog.add_pulse(0, pulse, qubit_freq=5.0e9)
                
                state = prog.state(backend="numpy")
                fidelity = abs(state[1])**2 if len(state) > 1 else 0
                
                status = "âœ… Good" if fidelity > 0.9 else "âš ï¸  OK" if fidelity > 0.8 else "âŒ Poor"
                print(f"{amp:<6.1f} {dur:<6.0f} {beta:<6.2f} {fidelity:<10.4f} {status:<15}")
                
                if fidelity > best_fidelity:
                    best_fidelity = fidelity
                    best_params = (amp, dur, beta)
    
    print(f"\nâœ… Best Parameters Found:")
    print(f"  Amplitude: {best_params[0]:.2f}")
    print(f"  Duration: {best_params[1]:.0f} ns")
    print(f"  Beta: {best_params[2]:.2f}")
    print(f"  Fidelity: {best_fidelity:.4f}")
    
    print("\nğŸ’¡ Optimization Insights:")
    print("  â€¢ Grid search: Good for exploration, expensive")
    print("  â€¢ Gradient descent: Good for refinement, local minima")
    print("  â€¢ Combined: Best fidelity with reasonable cost")
    
    print("\nâœ… Multi-parameter optimization complete!")


# ==============================================================================
# Summary
# ==============================================================================

def print_summary():
    """Print comprehensive summary."""
    print("\n" + "="*70)
    print("ğŸ“š Summary: Advanced Pulse Optimization")
    print("="*70)
    
    print("""
Optimization Techniques:

  1. Parameter Shift Rule (PSR):
     âœ… Works with any quantum circuit
     âœ… No model assumptions
     âœ… Hardware compatible
     âŒ O(n) circuit evaluations

  2. AutoGrad (Automatic Differentiation):
     âœ… One backward pass for all parameters
     âœ… Efficient for many parameters
     âœ… Better numerical accuracy
     âœ… Cleaner implementation
     âŒ Requires compatible backend

  3. Grid Search:
     âœ… Explores parameter space uniformly
     âœ… No gradient computation needed
     âœ… Identifies local optima
     âŒ Expensive for many parameters

  4. Gradient Descent:
     âœ… Efficient convergence
     âœ… Works with gradients
     âœ… Scales to many parameters
     âŒ Can get stuck in local minima

  5. Adam Optimizer:
     âœ… Adaptive learning rates
     âœ… Momentum and velocity
     âœ… Practical effectiveness
     âœ… Industry standard

Workflow for Pulse Optimization:

  Step 1: Define Objective Function
    â€¢ Choose metric (fidelity, leakage, etc.)
    â€¢ Implement cost = 1 - metric

  Step 2: Choose Parameters to Optimize
    â€¢ Amplitude: Controls rotation speed
    â€¢ Duration: Controls rotation angle
    â€¢ Shape: DRAG beta, envelope, etc.

  Step 3: Compute Gradients
    â€¢ Option A: Parameter shift rule
    â€¢ Option B: Automatic differentiation
    â€¢ Option C: Numerical finite differences

  Step 4: Update Parameters
    â€¢ Simple: Î¸_new = Î¸_old - lr * âˆ‡C
    â€¢ Better: Use Adam or other optimizer
    â€¢ Check: Convergence criteria

  Step 5: Validate
    â€¢ Test on independent data
    â€¢ Verify on real hardware
    â€¢ Monitor for overfitting

Best Practices:

  âœ… Start with simple gate (X or H)
  âœ… Use coarse grid search first
  âœ… Refine with gradient descent
  âœ… Validate with 3-level simulation
  âœ… Test on realistic noise
  âœ… Document optimal parameters

Common Pitfalls:

  âŒ Over-optimizing to simulation
  âŒ Ignoring hardware constraints
  âŒ Not validating on independent data
  âŒ Assuming optimization generalizes
  âŒ Ignoring computational cost

Next Steps:

  â†’ See pulse_variational_algorithms.py for algorithm optimization
  â†’ See pulse_gate_calibration.py for full calibration workflow
  â†’ See pulse_cloud_submission_e2e.py for deployment
""")


# ==============================================================================
# Main Entry Point
# ==============================================================================

if __name__ == "__main__":
    print("\n" + "="*70)
    print("ğŸ¯ TyxonQ Advanced Pulse Optimization")
    print("="*70)
    
    print("""
Master gradient-based pulse optimization techniques:

  â€¢ Parameter shift rule for gradient estimation
  â€¢ Automatic differentiation integration
  â€¢ Single-qubit gate optimization
  â€¢ Two-qubit gate optimization
  â€¢ Waveform shape selection
  â€¢ Multi-parameter simultaneous optimization
""")
    
    example_1_parameter_shift_rule()
    example_2_autograd()
    example_3_single_qubit_optimization()
    example_4_two_qubit_optimization()
    example_5_pulse_shape_search()
    example_6_multi_parameter_optimization()
    print_summary()
    
    print("\n" + "="*70)
    print("âœ… All Examples Complete!")
    print("="*70)
